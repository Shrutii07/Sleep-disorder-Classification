{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz_-AYRlssB8",
        "outputId": "e5e7e0ad-57dd-4669-a121-2ac01e1c14a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import tensorflow as tf\n",
        "# from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "CiKBS9fPwnN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import cross_val_score\n",
        "diabetes = datasets.load_diabetes()\n",
        "X = diabetes.data[:150]\n",
        "y = diabetes.target[:150]\n",
        "lasso = linear_model.Lasso()\n",
        "print(cross_val_score(lasso, X, y, cv=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhBO6aEYACR-",
        "outputId": "73997b73-28ad-42ff-fe2d-32c5be582443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.33150734 0.08022311 0.03531764]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model1 = tf.keras.models.load_model('/content/drive/MyDrive/ML/H&U_classification/lstm/model_v4.h5')\n",
        "#history=np.load('/content/drive/MyDrive/ML/H&U_classification/lstm/ver4_history.npy',allow_pickle='TRUE').item()\n",
        "\n",
        "x_test1 = np.loadtxt('/content/drive/MyDrive/ML/H&U_classification/lstm/xtest_data_ver4.csv', delimiter=',')\n",
        "y_test1 = np.loadtxt('/content/drive/MyDrive/ML/H&U_classification/lstm/ytest_data_ver4.csv', delimiter=',')\n",
        "x_train1 = np.loadtxt('/content/drive/MyDrive/ML/H&U_classification/lstm/xtrain_data_ver4.csv', delimiter=',')\n",
        "y_train1 = np.loadtxt('/content/drive/MyDrive/ML/H&U_classification/lstm/ytrain_data_ver4.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "Gkysiss1JpHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.layers as tfl\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy\n",
        "def create_model(param):\n",
        "    model = Sequential()\n",
        "    model.add(tfl.Conv1D(filters=32,\n",
        "                    kernel_size=7,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "            input_shape=(1024,1)))\n",
        "   \n",
        "    model.add(tfl.Conv1D(filters=8,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same', activation='relu'))\n",
        "    model.add(tfl.MaxPool1D(pool_size= 16,\n",
        "                       padding='same'))\n",
        "    model.add(tfl.Conv1D(filters=16,\n",
        "                    kernel_size=2,\n",
        "                    strides=1,\n",
        "                    padding='same'))\n",
        "    model.add(tfl.Dropout(0.4))\n",
        "    model.add(tfl.Conv1D(filters=8,\n",
        "                    kernel_size=2,\n",
        "                    strides=1,\n",
        "                    padding='same',activation='relu'))\n",
        "    model.add(tfl.MaxPool1D(pool_size= 4,\n",
        "                       padding='same'))\n",
        "    model.add(tfl.Flatten())\n",
        "    model.add(tfl.Dense(param, activation='relu'))\n",
        "    model.add(tfl.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer = 'adam',\n",
        "                  loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "n4DvyQUEef2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.layers as tfl\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy\n",
        "def create_model(param):\n",
        "    model = Sequential()\n",
        "    model.add(keras.Input(shape=(1024,1)))\n",
        "    model.add(Conv1D(filters=32,\n",
        "                        kernel_size=7,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    model.add(Conv1D(filters=16,\n",
        "                        kernel_size=9,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    # model.add(MaxPooling1D(pool_size= 16,\n",
        "                          #  padding='same'))\n",
        "    model.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling1D(pool_size=8))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_wxdv6B3JyyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import *\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=1) \n",
        "fold_no = 1\n",
        "acc_per_fold1 = []\n",
        "loss_per_fold1 = []\n",
        "for train, test in kfold.split(x_train1,  y_train1):\n",
        "    model1 = Sequential()\n",
        "    model1.add(keras.Input(shape=(1024,1)))\n",
        "    model1.add(Conv1D(filters=32,\n",
        "                        kernel_size=7,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    model1.add(Conv1D(filters=16,\n",
        "                        kernel_size=9,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    # model.add(MaxPooling1D(pool_size= 16,\n",
        "                          #  padding='same'))\n",
        "    model1.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling1D(pool_size=8))\n",
        "    model1.add(LSTM(100))                               ############### parameter\n",
        "    model1.add(Dense(1, activation='sigmoid'))\n",
        "    model1.compile(optimizer = 'adam',\n",
        "                  loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    \n",
        "    # Fit data to model\n",
        "    history1 = model1.fit(x_train1[train], y_train1[train],\n",
        "                batch_size=75,\n",
        "                epochs=50)\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores1 = model1.evaluate(x_train1[test], y_train1[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model1.metrics_names[0]} of {scores1[0]}; {model1.metrics_names[1]} of {scores1[1]*100}%')\n",
        "    acc_per_fold1.append(scores1[1] * 100)\n",
        "    loss_per_fold1.append(scores1[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM3jHGTuifAB",
        "outputId": "88d0305d-3bc7-4ad6-a57d-37ca25587a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 15s 159ms/step - loss: 0.6986 - accuracy: 0.5143\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6886 - accuracy: 0.5270\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6776 - accuracy: 0.5731\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6756 - accuracy: 0.5761\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6661 - accuracy: 0.6018\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6639 - accuracy: 0.5988\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6869 - accuracy: 0.5452\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6694 - accuracy: 0.5842\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6673 - accuracy: 0.5887\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6665 - accuracy: 0.5932\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6463 - accuracy: 0.6284\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6380 - accuracy: 0.6468\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6396 - accuracy: 0.6413\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6174 - accuracy: 0.6682\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.5957 - accuracy: 0.6981\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.5591 - accuracy: 0.7350\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.6016 - accuracy: 0.7038\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.5371 - accuracy: 0.7678\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.5050 - accuracy: 0.7848\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.4937 - accuracy: 0.7838\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.4643 - accuracy: 0.8101\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.4555 - accuracy: 0.8101\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3905 - accuracy: 0.8511\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3993 - accuracy: 0.8483\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.4054 - accuracy: 0.8439\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.4131 - accuracy: 0.8380\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3365 - accuracy: 0.8765\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3600 - accuracy: 0.8637\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3730 - accuracy: 0.8572\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3849 - accuracy: 0.8570\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3581 - accuracy: 0.8681\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3471 - accuracy: 0.8745\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3512 - accuracy: 0.8720\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 13s 166ms/step - loss: 0.3198 - accuracy: 0.8802\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 13s 162ms/step - loss: 0.3055 - accuracy: 0.8884\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2980 - accuracy: 0.8893\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2933 - accuracy: 0.8913\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2880 - accuracy: 0.8911\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2794 - accuracy: 0.8960\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2875 - accuracy: 0.8908\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2644 - accuracy: 0.9026\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2640 - accuracy: 0.9031\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2648 - accuracy: 0.9015\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2565 - accuracy: 0.9046\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2615 - accuracy: 0.9007\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2681 - accuracy: 0.8973\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2592 - accuracy: 0.9005\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2456 - accuracy: 0.9076\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2468 - accuracy: 0.9061\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2375 - accuracy: 0.9071\n",
            "Score for fold 1: loss of 0.2632414400577545; accuracy of 90.18816947937012%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 15s 159ms/step - loss: 0.6965 - accuracy: 0.5089\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6939 - accuracy: 0.5255\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6886 - accuracy: 0.5355\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6779 - accuracy: 0.5669\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6747 - accuracy: 0.5738\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6658 - accuracy: 0.5980\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6654 - accuracy: 0.6033\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6491 - accuracy: 0.6168\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6481 - accuracy: 0.6223\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6521 - accuracy: 0.6085\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6585 - accuracy: 0.6090\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6736 - accuracy: 0.5754\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6731 - accuracy: 0.5837\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.6517 - accuracy: 0.6274\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6339 - accuracy: 0.6499\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.6296 - accuracy: 0.6472\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6332 - accuracy: 0.6507\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.5842 - accuracy: 0.7048\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.5537 - accuracy: 0.7438\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.4972 - accuracy: 0.7760\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.4684 - accuracy: 0.7962\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.4913 - accuracy: 0.7809\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.4643 - accuracy: 0.8085\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.4589 - accuracy: 0.8185\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.5552 - accuracy: 0.7278\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6237 - accuracy: 0.6779\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.5019 - accuracy: 0.7952\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.4975 - accuracy: 0.8002\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.4807 - accuracy: 0.8061\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3949 - accuracy: 0.8506\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.4411 - accuracy: 0.8059\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.5440 - accuracy: 0.7609\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.4014 - accuracy: 0.8565\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3771 - accuracy: 0.8589\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3357 - accuracy: 0.8832\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3287 - accuracy: 0.8846\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3272 - accuracy: 0.8799\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3162 - accuracy: 0.8859\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3194 - accuracy: 0.8836\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3092 - accuracy: 0.8842\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3124 - accuracy: 0.8812\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.3028 - accuracy: 0.8861\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2959 - accuracy: 0.8878\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2903 - accuracy: 0.8881\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2942 - accuracy: 0.8852\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2811 - accuracy: 0.8906\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2745 - accuracy: 0.8943\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2677 - accuracy: 0.8982\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 13s 157ms/step - loss: 0.2732 - accuracy: 0.8953\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2664 - accuracy: 0.8945\n",
            "Score for fold 2: loss of 0.25652632117271423; accuracy of 89.58333134651184%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 15s 158ms/step - loss: 0.6972 - accuracy: 0.5118\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6830 - accuracy: 0.5546\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6746 - accuracy: 0.5722\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6748 - accuracy: 0.5702\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6782 - accuracy: 0.5680\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6700 - accuracy: 0.5882\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6651 - accuracy: 0.5949\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6727 - accuracy: 0.5749\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6495 - accuracy: 0.6235\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6665 - accuracy: 0.5934\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6757 - accuracy: 0.5665\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6727 - accuracy: 0.5845\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6660 - accuracy: 0.6010\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6660 - accuracy: 0.5973\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6739 - accuracy: 0.5731\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6604 - accuracy: 0.6075\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6386 - accuracy: 0.6455\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6354 - accuracy: 0.6546\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6175 - accuracy: 0.6791\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6058 - accuracy: 0.6897\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5941 - accuracy: 0.6954\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5791 - accuracy: 0.7098\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5479 - accuracy: 0.7329\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6051 - accuracy: 0.6766\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5544 - accuracy: 0.7231\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.4863 - accuracy: 0.7765\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.4385 - accuracy: 0.8234\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3928 - accuracy: 0.8476\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3730 - accuracy: 0.8579\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3609 - accuracy: 0.8604\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3211 - accuracy: 0.8772\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3105 - accuracy: 0.8804\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2824 - accuracy: 0.8889\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2769 - accuracy: 0.8888\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2647 - accuracy: 0.8972\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2730 - accuracy: 0.8884\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2513 - accuracy: 0.9002\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2389 - accuracy: 0.9039\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2257 - accuracy: 0.9105\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2109 - accuracy: 0.9141\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.1910 - accuracy: 0.9262\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2047 - accuracy: 0.9182\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2027 - accuracy: 0.9190\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.1917 - accuracy: 0.9261\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.1810 - accuracy: 0.9254\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.1766 - accuracy: 0.9306\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.1750 - accuracy: 0.9304\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2114 - accuracy: 0.9130\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.1607 - accuracy: 0.9368\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.1466 - accuracy: 0.9409\n",
            "Score for fold 3: loss of 0.21064546704292297; accuracy of 91.5994644165039%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 15s 160ms/step - loss: 0.6980 - accuracy: 0.5116\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6936 - accuracy: 0.5202\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6934 - accuracy: 0.5307\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6859 - accuracy: 0.5494\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6791 - accuracy: 0.5670\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6858 - accuracy: 0.5437\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6697 - accuracy: 0.5828\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6673 - accuracy: 0.5848\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6679 - accuracy: 0.5885\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.6598 - accuracy: 0.6038\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6623 - accuracy: 0.5927\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6648 - accuracy: 0.6018\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6764 - accuracy: 0.5692\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6727 - accuracy: 0.5813\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6615 - accuracy: 0.5964\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6511 - accuracy: 0.6215\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6439 - accuracy: 0.6415\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6241 - accuracy: 0.6687\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6174 - accuracy: 0.6769\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6130 - accuracy: 0.6783\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6071 - accuracy: 0.6858\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6018 - accuracy: 0.6904\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5940 - accuracy: 0.6935\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.5874 - accuracy: 0.6993\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.5797 - accuracy: 0.7040\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5635 - accuracy: 0.7186\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5376 - accuracy: 0.7359\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.4865 - accuracy: 0.7839\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.4233 - accuracy: 0.8327\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.4099 - accuracy: 0.8407\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3667 - accuracy: 0.8626\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3544 - accuracy: 0.8673\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3408 - accuracy: 0.8701\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3432 - accuracy: 0.8690\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3213 - accuracy: 0.8817\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2913 - accuracy: 0.8905\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3206 - accuracy: 0.8748\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3032 - accuracy: 0.8831\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2668 - accuracy: 0.8984\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2470 - accuracy: 0.9012\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2653 - accuracy: 0.8940\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2642 - accuracy: 0.8943\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.2558 - accuracy: 0.8963\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.2144 - accuracy: 0.9096\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.2228 - accuracy: 0.9068\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.2145 - accuracy: 0.9136\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.2240 - accuracy: 0.9079\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.1964 - accuracy: 0.9215\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.1924 - accuracy: 0.9264\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.1924 - accuracy: 0.9202\n",
            "Score for fold 4: loss of 0.22364060580730438; accuracy of 91.3306474685669%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 15s 161ms/step - loss: 0.6991 - accuracy: 0.5096\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6827 - accuracy: 0.5536\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6785 - accuracy: 0.5578\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6694 - accuracy: 0.5921\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6601 - accuracy: 0.6042\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6531 - accuracy: 0.6168\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6588 - accuracy: 0.6018\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6511 - accuracy: 0.6164\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6884 - accuracy: 0.5415\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6684 - accuracy: 0.5872\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6497 - accuracy: 0.6242\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6344 - accuracy: 0.6549\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6327 - accuracy: 0.6522\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6283 - accuracy: 0.6573\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6257 - accuracy: 0.6599\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6306 - accuracy: 0.6470\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6229 - accuracy: 0.6680\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6264 - accuracy: 0.6623\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6171 - accuracy: 0.6799\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.6168 - accuracy: 0.6725\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6065 - accuracy: 0.6811\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.6024 - accuracy: 0.6846\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5965 - accuracy: 0.6885\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.5872 - accuracy: 0.7006\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5811 - accuracy: 0.7023\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5734 - accuracy: 0.7120\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5676 - accuracy: 0.7100\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5654 - accuracy: 0.7124\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.5467 - accuracy: 0.7293\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.5318 - accuracy: 0.7350\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.5176 - accuracy: 0.7584\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.4505 - accuracy: 0.8080\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3918 - accuracy: 0.8456\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3710 - accuracy: 0.8543\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3544 - accuracy: 0.8644\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3459 - accuracy: 0.8671\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 13s 158ms/step - loss: 0.3220 - accuracy: 0.8780\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3125 - accuracy: 0.8836\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.3207 - accuracy: 0.8772\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3129 - accuracy: 0.8807\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.3002 - accuracy: 0.8864\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2840 - accuracy: 0.8913\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2804 - accuracy: 0.8978\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.2691 - accuracy: 0.9000\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.2866 - accuracy: 0.8920\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2714 - accuracy: 0.8970\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2650 - accuracy: 0.9010\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 13s 159ms/step - loss: 0.2706 - accuracy: 0.8982\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.2647 - accuracy: 0.9005\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 13s 160ms/step - loss: 0.2704 - accuracy: 0.8973\n",
            "Score for fold 5: loss of 0.2802598476409912; accuracy of 89.51612710952759%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold1)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold1[i]} - Accuracy: {acc_per_fold1[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold1)} (+- {np.std(acc_per_fold1)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold1)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "np.savetxt('/content/drive/MyDrive/ML/Hyper_opti/dense_param100_50.csv', acc_per_fold1, delimiter=',')"
      ],
      "metadata": {
        "id": "D2tj2DjGjAKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48ed9f0-e249-46cb-bca4-cb79fe029336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.2632414400577545 - Accuracy: 90.18816947937012%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.25652632117271423 - Accuracy: 89.58333134651184%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.21064546704292297 - Accuracy: 91.5994644165039%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.22364060580730438 - Accuracy: 91.3306474685669%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2802598476409912 - Accuracy: 89.51612710952759%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 90.44354796409607 (+- 0.8704468527023358)\n",
            "> Loss: 0.24686273634433747\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import *\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=1) \n",
        "fold_no = 1\n",
        "acc_per_fold2 = []\n",
        "loss_per_fold2 = []\n",
        "for train, test in kfold.split(x_train1,y_train1):\n",
        "    model2 = Sequential()\n",
        "    model2.add(keras.Input(shape=(1024,1)))\n",
        "    model2.add(Conv1D(filters=32,\n",
        "                        kernel_size=7,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    model2.add(Conv1D(filters=16,\n",
        "                        kernel_size=9,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    # model.add(MaxPooling1D(pool_size= 16,\n",
        "                          #  padding='same'))\n",
        "    model2.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling1D(pool_size=8))\n",
        "    model2.add(LSTM(16))\n",
        "    model2.add(Dense(1, activation='sigmoid'))\n",
        "    model2.compile(optimizer = 'adam',\n",
        "                  loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    \n",
        "    # Fit data to model\n",
        "    history2 = model2.fit(x_train1[train], y_train1[train],\n",
        "                batch_size=75,\n",
        "                epochs=50)\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores2 = model2.evaluate(x_train1[test], y_train1[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model2.metrics_names[0]} of {scores2[0]}; {model2.metrics_names[1]} of {scores2[1]*100}%')\n",
        "    acc_per_fold2.append(scores2[1] * 100)\n",
        "    loss_per_fold2.append(scores2[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "ytCC3HtVFQH4",
        "outputId": "c4510fda-ee4f-4aa0-cfe6-3213e3347a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 66s 792ms/step - loss: 0.6955 - accuracy: 0.5151\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 61s 757ms/step - loss: 0.6920 - accuracy: 0.5156\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 55s 685ms/step - loss: 0.6902 - accuracy: 0.5265\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 53s 663ms/step - loss: 0.6867 - accuracy: 0.5439\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 53s 658ms/step - loss: 0.6844 - accuracy: 0.5460\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 53s 664ms/step - loss: 0.6818 - accuracy: 0.5623\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 54s 673ms/step - loss: 0.6789 - accuracy: 0.5662\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 54s 672ms/step - loss: 0.6737 - accuracy: 0.5848\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 54s 671ms/step - loss: 0.6702 - accuracy: 0.5811\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 53s 667ms/step - loss: 0.6680 - accuracy: 0.5869\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 54s 669ms/step - loss: 0.6665 - accuracy: 0.5944\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 53s 666ms/step - loss: 0.6509 - accuracy: 0.6260\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 54s 669ms/step - loss: 0.6106 - accuracy: 0.6877\n",
            "Epoch 14/50\n",
            "62/80 [======================>.......] - ETA: 11s - loss: 0.5765 - accuracy: 0.7335"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ea4cf044caeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     history2 = model2.fit(x_train1[train], y_train1[train],\n\u001b[1;32m     45\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 epochs=50)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Generate generalization metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1370\u001b[0m           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[1;32m   1371\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold2)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold2[i]} - Accuracy: {acc_per_fold2[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold2)} (+- {np.std(acc_per_fold2)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold2)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "np.savetxt('/content/drive/MyDrive/ML/Hyper_opti/lstm_param16_50.csv', acc_per_fold2, delimiter=',')"
      ],
      "metadata": {
        "id": "iLUGUMc2H13Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import *\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=1) \n",
        "fold_no = 1\n",
        "acc_per_fold3 = []\n",
        "loss_per_fold3 = []\n",
        "for train, test in kfold.split(x_train1,y_train1):\n",
        "    model3 = Sequential()\n",
        "    model3.add(keras.Input(shape=(1024,1)))\n",
        "    model3.add(Conv1D(filters=32,\n",
        "                        kernel_size=7,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    model3.add(Conv1D(filters=16,\n",
        "                        kernel_size=9,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    # model.add(MaxPooling1D(pool_size= 16,\n",
        "                          #  padding='same'))\n",
        "    model3.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling1D(pool_size=8))\n",
        "    model3.add(LSTM(128))\n",
        "    model3.add(Dense(1, activation='sigmoid'))\n",
        "    model3.compile(optimizer = 'adam',\n",
        "                  loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    \n",
        "    # Fit data to model\n",
        "    history3 = model3.fit(x_train1[train], y_train1[train],\n",
        "                batch_size=75,\n",
        "                epochs=50)\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores3 = model3.evaluate(x_train1[test], y_train1[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model3.metrics_names[0]} of {scores3[0]}; {model3.metrics_names[1]} of {scores3[1]*100}%')\n",
        "    acc_per_fold3.append(scores3[1] * 100)\n",
        "    loss_per_fold3.append(scores3[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6MenH8yJkZ8",
        "outputId": "e0ab1d32-1ef1-47c3-d033-2e3ddb31be67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 16s 176ms/step - loss: 0.7018 - accuracy: 0.5143\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6853 - accuracy: 0.5465\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6742 - accuracy: 0.5790\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6718 - accuracy: 0.5820\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6617 - accuracy: 0.5959\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.6685 - accuracy: 0.5963\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6793 - accuracy: 0.5654\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6721 - accuracy: 0.5793\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6639 - accuracy: 0.5983\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6639 - accuracy: 0.5922\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6477 - accuracy: 0.6203\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.6579 - accuracy: 0.6094\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6483 - accuracy: 0.6287\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.6521 - accuracy: 0.6178\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6273 - accuracy: 0.6536\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.6289 - accuracy: 0.6571\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6143 - accuracy: 0.6665\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6184 - accuracy: 0.6630\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6153 - accuracy: 0.6613\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.6024 - accuracy: 0.6752\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5756 - accuracy: 0.6961\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.5768 - accuracy: 0.7194\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5486 - accuracy: 0.7453\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.4907 - accuracy: 0.7912\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.4645 - accuracy: 0.8073\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.4666 - accuracy: 0.7947\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3849 - accuracy: 0.8557\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.4025 - accuracy: 0.8448\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3638 - accuracy: 0.8621\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.3358 - accuracy: 0.8740\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3328 - accuracy: 0.8768\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3329 - accuracy: 0.8718\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3127 - accuracy: 0.8795\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.2931 - accuracy: 0.8873\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2918 - accuracy: 0.8883\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2704 - accuracy: 0.8943\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2647 - accuracy: 0.8973\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3110 - accuracy: 0.8765\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2791 - accuracy: 0.8923\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2566 - accuracy: 0.9020\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2442 - accuracy: 0.9066\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.2464 - accuracy: 0.9041\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2530 - accuracy: 0.9000\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.2424 - accuracy: 0.9083\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2465 - accuracy: 0.9027\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2459 - accuracy: 0.9024\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2243 - accuracy: 0.9106\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2109 - accuracy: 0.9135\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2070 - accuracy: 0.9173\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2012 - accuracy: 0.9172\n",
            "Score for fold 1: loss of 0.22350606322288513; accuracy of 90.52419066429138%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 17s 175ms/step - loss: 0.6995 - accuracy: 0.5076\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6930 - accuracy: 0.5294\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6828 - accuracy: 0.5460\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6733 - accuracy: 0.5704\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6648 - accuracy: 0.5978\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6666 - accuracy: 0.5963\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6527 - accuracy: 0.6190\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6515 - accuracy: 0.6211\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6515 - accuracy: 0.6191\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6473 - accuracy: 0.6218\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6534 - accuracy: 0.6245\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6456 - accuracy: 0.6287\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6224 - accuracy: 0.6620\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.6292 - accuracy: 0.6499\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6665 - accuracy: 0.5892\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6487 - accuracy: 0.6178\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6451 - accuracy: 0.6284\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6461 - accuracy: 0.6319\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6674 - accuracy: 0.5853\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6355 - accuracy: 0.6569\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6405 - accuracy: 0.6489\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6187 - accuracy: 0.6762\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6075 - accuracy: 0.6873\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5988 - accuracy: 0.6949\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5928 - accuracy: 0.7021\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5862 - accuracy: 0.7016\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5838 - accuracy: 0.6984\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5720 - accuracy: 0.7117\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5651 - accuracy: 0.7045\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5608 - accuracy: 0.7092\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5149 - accuracy: 0.7552\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.4988 - accuracy: 0.7747\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.4806 - accuracy: 0.7917\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3918 - accuracy: 0.8523\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3974 - accuracy: 0.8461\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3649 - accuracy: 0.8580\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3651 - accuracy: 0.8683\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3431 - accuracy: 0.8708\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3481 - accuracy: 0.8664\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3190 - accuracy: 0.8787\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3122 - accuracy: 0.8829\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3080 - accuracy: 0.8805\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2998 - accuracy: 0.8842\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2895 - accuracy: 0.8911\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3088 - accuracy: 0.8787\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3139 - accuracy: 0.8785\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3028 - accuracy: 0.8849\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2873 - accuracy: 0.8911\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2816 - accuracy: 0.8970\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2757 - accuracy: 0.8930\n",
            "Score for fold 2: loss of 0.2819095551967621; accuracy of 89.6505355834961%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 16s 176ms/step - loss: 0.7028 - accuracy: 0.5034\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6921 - accuracy: 0.5193\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6843 - accuracy: 0.5449\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6762 - accuracy: 0.5726\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6775 - accuracy: 0.5662\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6757 - accuracy: 0.5640\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6643 - accuracy: 0.5877\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6530 - accuracy: 0.6117\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6503 - accuracy: 0.6198\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6670 - accuracy: 0.5894\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6556 - accuracy: 0.6048\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6456 - accuracy: 0.6168\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6541 - accuracy: 0.6154\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6629 - accuracy: 0.6013\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6684 - accuracy: 0.5890\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6495 - accuracy: 0.6263\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6429 - accuracy: 0.6359\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6437 - accuracy: 0.6485\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6211 - accuracy: 0.6783\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6160 - accuracy: 0.6806\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6042 - accuracy: 0.6872\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5936 - accuracy: 0.6920\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5608 - accuracy: 0.7256\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 0.5342 - accuracy: 0.7416\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.4790 - accuracy: 0.7987\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.4090 - accuracy: 0.8419\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3861 - accuracy: 0.8569\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3613 - accuracy: 0.8649\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3444 - accuracy: 0.8690\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3315 - accuracy: 0.8737\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3238 - accuracy: 0.8768\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3249 - accuracy: 0.8774\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2865 - accuracy: 0.8930\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2840 - accuracy: 0.8905\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2732 - accuracy: 0.8900\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2686 - accuracy: 0.8975\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2862 - accuracy: 0.8863\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2465 - accuracy: 0.9046\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2421 - accuracy: 0.9024\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2219 - accuracy: 0.9125\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2122 - accuracy: 0.9160\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2135 - accuracy: 0.9162\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2098 - accuracy: 0.9123\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.1957 - accuracy: 0.9189\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.1904 - accuracy: 0.9274\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.1869 - accuracy: 0.9247\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.1712 - accuracy: 0.9328\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.1688 - accuracy: 0.9323\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.1660 - accuracy: 0.9348\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.1524 - accuracy: 0.9409\n",
            "Score for fold 3: loss of 0.2124524712562561; accuracy of 91.06183052062988%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 16s 175ms/step - loss: 0.6968 - accuracy: 0.5245\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6871 - accuracy: 0.5417\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6835 - accuracy: 0.5559\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6780 - accuracy: 0.5696\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6706 - accuracy: 0.5840\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6694 - accuracy: 0.5897\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6635 - accuracy: 0.5944\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6514 - accuracy: 0.6179\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6640 - accuracy: 0.5973\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6597 - accuracy: 0.6090\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6797 - accuracy: 0.5701\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6556 - accuracy: 0.6112\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6460 - accuracy: 0.6205\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6329 - accuracy: 0.6445\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6323 - accuracy: 0.6500\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6282 - accuracy: 0.6526\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6194 - accuracy: 0.6601\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6127 - accuracy: 0.6673\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6062 - accuracy: 0.6773\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5973 - accuracy: 0.6784\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6258 - accuracy: 0.6426\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5923 - accuracy: 0.6853\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5905 - accuracy: 0.6811\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5420 - accuracy: 0.7295\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.4897 - accuracy: 0.7839\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.4261 - accuracy: 0.8337\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.4297 - accuracy: 0.8165\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.4021 - accuracy: 0.8399\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3711 - accuracy: 0.8565\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3477 - accuracy: 0.8684\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3190 - accuracy: 0.8795\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3284 - accuracy: 0.8789\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3578 - accuracy: 0.8604\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3177 - accuracy: 0.8774\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3079 - accuracy: 0.8837\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2971 - accuracy: 0.8856\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3045 - accuracy: 0.8817\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2769 - accuracy: 0.8950\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2599 - accuracy: 0.8958\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2524 - accuracy: 0.9004\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2221 - accuracy: 0.9133\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2364 - accuracy: 0.9052\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 14s 176ms/step - loss: 0.2071 - accuracy: 0.9207\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2060 - accuracy: 0.9190\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.2172 - accuracy: 0.9202\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2057 - accuracy: 0.9162\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.2022 - accuracy: 0.9205\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.1852 - accuracy: 0.9288\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.1859 - accuracy: 0.9249\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 14s 176ms/step - loss: 0.1826 - accuracy: 0.9224\n",
            "Score for fold 4: loss of 0.5700452923774719; accuracy of 75.40322542190552%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 17s 177ms/step - loss: 0.7002 - accuracy: 0.5146\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6897 - accuracy: 0.5380\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 14s 176ms/step - loss: 0.6843 - accuracy: 0.5549\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6820 - accuracy: 0.5580\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6858 - accuracy: 0.5502\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6738 - accuracy: 0.5768\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6676 - accuracy: 0.5877\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6736 - accuracy: 0.5827\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 14s 176ms/step - loss: 0.6639 - accuracy: 0.6020\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6552 - accuracy: 0.6186\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6516 - accuracy: 0.6139\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6491 - accuracy: 0.6215\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6600 - accuracy: 0.6015\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6564 - accuracy: 0.6079\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6464 - accuracy: 0.6226\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6465 - accuracy: 0.6257\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6417 - accuracy: 0.6268\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6754 - accuracy: 0.5800\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6760 - accuracy: 0.5781\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6623 - accuracy: 0.6090\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6543 - accuracy: 0.6176\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6440 - accuracy: 0.6265\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6348 - accuracy: 0.6472\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6350 - accuracy: 0.6458\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6370 - accuracy: 0.6383\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6437 - accuracy: 0.6294\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6337 - accuracy: 0.6445\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6120 - accuracy: 0.6799\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5979 - accuracy: 0.6899\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6464 - accuracy: 0.6403\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5966 - accuracy: 0.7174\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6279 - accuracy: 0.6709\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5660 - accuracy: 0.7419\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5796 - accuracy: 0.7164\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6642 - accuracy: 0.5796\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.6419 - accuracy: 0.6208\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.6136 - accuracy: 0.6672\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5625 - accuracy: 0.7179\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.5042 - accuracy: 0.7663\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5034 - accuracy: 0.7624\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.4472 - accuracy: 0.8180\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.5145 - accuracy: 0.7641\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.4215 - accuracy: 0.8416\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3920 - accuracy: 0.8453\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3785 - accuracy: 0.8574\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3628 - accuracy: 0.8595\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3550 - accuracy: 0.8653\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3409 - accuracy: 0.8726\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 14s 175ms/step - loss: 0.3256 - accuracy: 0.8757\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 14s 174ms/step - loss: 0.3428 - accuracy: 0.8713\n",
            "Score for fold 5: loss of 0.30838850140571594; accuracy of 88.70967626571655%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold3)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold3[i]} - Accuracy: {acc_per_fold3[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold3)} (+- {np.std(acc_per_fold3)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold3)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "np.savetxt('/content/drive/MyDrive/ML/Hyper_opti/dense_param128_50.csv', acc_per_fold3, delimiter=',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Peo6fXYxK3z0",
        "outputId": "4db8d2ae-835c-440d-f77b-6cad80724e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.22350606322288513 - Accuracy: 90.52419066429138%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.2819095551967621 - Accuracy: 89.6505355834961%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.2124524712562561 - Accuracy: 91.06183052062988%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.5700452923774719 - Accuracy: 75.40322542190552%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.30838850140571594 - Accuracy: 88.70967626571655%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 87.06989169120789 (+- 5.887741081300559)\n",
            "> Loss: 0.3192603766918182\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import *\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=1) \n",
        "fold_no = 1\n",
        "acc_per_fold4 = []\n",
        "loss_per_fold4 = []\n",
        "for train, test in kfold.split(x_train1,y_train1):\n",
        "    model4 = Sequential()\n",
        "    model4.add(keras.Input(shape=(1024,1)))\n",
        "    model4.add(Conv1D(filters=32,\n",
        "                        kernel_size=7,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    model4.add(Conv1D(filters=16,\n",
        "                        kernel_size=9,\n",
        "                        strides=1,\n",
        "                        padding='same'))\n",
        "    # model.add(MaxPooling1D(pool_size= 16,\n",
        "                          #  padding='same'))\n",
        "    model4.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling1D(pool_size=8))\n",
        "    model4.add(LSTM(256))\n",
        "    model4.add(Dense(1, activation='sigmoid'))\n",
        "    model4.compile(optimizer = 'adam',\n",
        "                  loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    \n",
        "    # Fit data to model\n",
        "    history4 = model4.fit(x_train1[train], y_train1[train],\n",
        "                batch_size=75,\n",
        "                epochs=50)\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores4 = model4.evaluate(x_train1[test], y_train1[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model4.metrics_names[0]} of {scores4[0]}; {model4.metrics_names[1]} of {scores4[1]*100}%')\n",
        "    acc_per_fold4.append(scores4[1] * 100)\n",
        "    loss_per_fold4.append(scores4[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLv6D_jRLBhW",
        "outputId": "40ce7e84-7dee-4a55-8791-5c79d9be72f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 25s 283ms/step - loss: 0.7033 - accuracy: 0.5102\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6911 - accuracy: 0.5420\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.6811 - accuracy: 0.5612\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6750 - accuracy: 0.5706\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6756 - accuracy: 0.5689\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6555 - accuracy: 0.6097\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.6597 - accuracy: 0.5973\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6779 - accuracy: 0.5541\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6727 - accuracy: 0.5759\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6601 - accuracy: 0.5968\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 22s 281ms/step - loss: 0.6555 - accuracy: 0.6161\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6547 - accuracy: 0.6139\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6475 - accuracy: 0.6359\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6382 - accuracy: 0.6391\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.6251 - accuracy: 0.6490\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6221 - accuracy: 0.6531\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6353 - accuracy: 0.6492\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6149 - accuracy: 0.6685\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6020 - accuracy: 0.6767\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5927 - accuracy: 0.6845\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5828 - accuracy: 0.6929\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5734 - accuracy: 0.6967\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5412 - accuracy: 0.7233\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.4912 - accuracy: 0.7750\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.4227 - accuracy: 0.8296\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 22s 281ms/step - loss: 0.3608 - accuracy: 0.8619\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.4087 - accuracy: 0.8340\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.4162 - accuracy: 0.8291\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.3732 - accuracy: 0.8585\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.3611 - accuracy: 0.8607\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 22s 281ms/step - loss: 0.3569 - accuracy: 0.8641\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.3274 - accuracy: 0.8740\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.3139 - accuracy: 0.8824\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.3032 - accuracy: 0.8852\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 22s 281ms/step - loss: 0.3072 - accuracy: 0.8831\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.3106 - accuracy: 0.8810\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2878 - accuracy: 0.8948\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2819 - accuracy: 0.8935\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2939 - accuracy: 0.8868\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2721 - accuracy: 0.8982\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2655 - accuracy: 0.8994\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2769 - accuracy: 0.8918\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.2450 - accuracy: 0.9066\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2564 - accuracy: 0.8992\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2557 - accuracy: 0.8992\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2310 - accuracy: 0.9093\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2251 - accuracy: 0.9088\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2296 - accuracy: 0.9079\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2233 - accuracy: 0.9101\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2210 - accuracy: 0.9106\n",
            "Score for fold 1: loss of 0.2634943425655365; accuracy of 89.11290168762207%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 25s 284ms/step - loss: 0.7016 - accuracy: 0.5128\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6904 - accuracy: 0.5351\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6849 - accuracy: 0.5502\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6728 - accuracy: 0.5827\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6648 - accuracy: 0.5963\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6723 - accuracy: 0.5768\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6638 - accuracy: 0.6038\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6596 - accuracy: 0.6010\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6443 - accuracy: 0.6300\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6504 - accuracy: 0.6248\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6615 - accuracy: 0.6058\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6559 - accuracy: 0.6179\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6791 - accuracy: 0.5664\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6656 - accuracy: 0.5974\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6693 - accuracy: 0.5820\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6568 - accuracy: 0.6084\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6558 - accuracy: 0.5988\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6851 - accuracy: 0.5533\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6911 - accuracy: 0.5306\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6839 - accuracy: 0.5501\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6626 - accuracy: 0.6064\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6513 - accuracy: 0.6238\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6602 - accuracy: 0.6047\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6685 - accuracy: 0.5969\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6521 - accuracy: 0.6141\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6520 - accuracy: 0.6126\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6491 - accuracy: 0.6190\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6545 - accuracy: 0.6109\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6407 - accuracy: 0.6324\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6434 - accuracy: 0.6242\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6366 - accuracy: 0.6484\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6422 - accuracy: 0.6321\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6110 - accuracy: 0.6762\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6130 - accuracy: 0.6731\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6083 - accuracy: 0.6784\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6037 - accuracy: 0.6816\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5975 - accuracy: 0.6761\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6637 - accuracy: 0.6050\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6486 - accuracy: 0.6376\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6220 - accuracy: 0.6722\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6046 - accuracy: 0.6924\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.5612 - accuracy: 0.7438\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5382 - accuracy: 0.7582\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.4914 - accuracy: 0.7875\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.4360 - accuracy: 0.8313\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.4527 - accuracy: 0.8160\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.4016 - accuracy: 0.8547\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3911 - accuracy: 0.8496\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.3428 - accuracy: 0.8730\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.4004 - accuracy: 0.8468\n",
            "Score for fold 2: loss of 0.354265958070755; accuracy of 86.82795763015747%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 25s 283ms/step - loss: 0.7020 - accuracy: 0.5210\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6921 - accuracy: 0.5267\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6831 - accuracy: 0.5462\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6730 - accuracy: 0.5694\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6650 - accuracy: 0.5848\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6508 - accuracy: 0.6146\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6597 - accuracy: 0.6055\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6611 - accuracy: 0.6052\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6735 - accuracy: 0.5906\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6614 - accuracy: 0.6112\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6872 - accuracy: 0.5324\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6781 - accuracy: 0.5655\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6738 - accuracy: 0.5696\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6697 - accuracy: 0.5857\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6518 - accuracy: 0.6262\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6365 - accuracy: 0.6484\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6257 - accuracy: 0.6598\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6077 - accuracy: 0.6774\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.5974 - accuracy: 0.6863\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.5981 - accuracy: 0.6794\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.5833 - accuracy: 0.6941\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.5821 - accuracy: 0.6925\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.5657 - accuracy: 0.7135\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5662 - accuracy: 0.7055\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5451 - accuracy: 0.7186\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.4901 - accuracy: 0.7695\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.4006 - accuracy: 0.8485\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3816 - accuracy: 0.8522\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.3358 - accuracy: 0.8753\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3219 - accuracy: 0.8824\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3006 - accuracy: 0.8905\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2895 - accuracy: 0.8928\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2880 - accuracy: 0.8913\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2638 - accuracy: 0.9010\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2609 - accuracy: 0.9007\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2542 - accuracy: 0.9047\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2449 - accuracy: 0.9084\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2504 - accuracy: 0.9047\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2534 - accuracy: 0.9002\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2472 - accuracy: 0.9034\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2169 - accuracy: 0.9136\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2154 - accuracy: 0.9136\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2135 - accuracy: 0.9153\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2061 - accuracy: 0.9162\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2003 - accuracy: 0.9219\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2014 - accuracy: 0.9214\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.1857 - accuracy: 0.9234\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.1861 - accuracy: 0.9205\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.1720 - accuracy: 0.9269\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.1574 - accuracy: 0.9345\n",
            "Score for fold 3: loss of 0.2541850209236145; accuracy of 89.85214829444885%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 25s 284ms/step - loss: 0.7026 - accuracy: 0.5102\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6946 - accuracy: 0.5247\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6806 - accuracy: 0.5643\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6806 - accuracy: 0.5691\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6692 - accuracy: 0.5827\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6621 - accuracy: 0.6013\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6617 - accuracy: 0.5890\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6719 - accuracy: 0.5776\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6750 - accuracy: 0.5719\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6759 - accuracy: 0.5746\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6563 - accuracy: 0.6079\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6454 - accuracy: 0.6334\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6678 - accuracy: 0.5919\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6688 - accuracy: 0.5904\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6433 - accuracy: 0.6452\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6311 - accuracy: 0.6645\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6162 - accuracy: 0.6771\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.6249 - accuracy: 0.6581\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6071 - accuracy: 0.6705\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5937 - accuracy: 0.6828\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6154 - accuracy: 0.6520\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5827 - accuracy: 0.6809\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5796 - accuracy: 0.6823\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5862 - accuracy: 0.6751\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5605 - accuracy: 0.7107\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5355 - accuracy: 0.7394\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.5278 - accuracy: 0.7446\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.4394 - accuracy: 0.8154\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3801 - accuracy: 0.8600\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.3504 - accuracy: 0.8671\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3655 - accuracy: 0.8612\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3403 - accuracy: 0.8691\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.3225 - accuracy: 0.8760\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.3109 - accuracy: 0.8810\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2958 - accuracy: 0.8893\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2781 - accuracy: 0.8957\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2739 - accuracy: 0.8970\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2680 - accuracy: 0.8990\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2556 - accuracy: 0.9042\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2464 - accuracy: 0.9029\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2414 - accuracy: 0.9054\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.2479 - accuracy: 0.9056\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2362 - accuracy: 0.9103\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2245 - accuracy: 0.9086\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2211 - accuracy: 0.9094\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2141 - accuracy: 0.9130\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.2007 - accuracy: 0.9167\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2133 - accuracy: 0.9113\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.1841 - accuracy: 0.9246\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.1697 - accuracy: 0.9278\n",
            "Score for fold 4: loss of 0.19367565214633942; accuracy of 92.6075279712677%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 25s 283ms/step - loss: 0.7106 - accuracy: 0.4919\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6942 - accuracy: 0.5205\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6853 - accuracy: 0.5514\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6768 - accuracy: 0.5632\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6694 - accuracy: 0.5865\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6641 - accuracy: 0.5983\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 23s 281ms/step - loss: 0.6657 - accuracy: 0.5973\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6564 - accuracy: 0.6099\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6475 - accuracy: 0.6220\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6833 - accuracy: 0.5501\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6740 - accuracy: 0.5773\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6585 - accuracy: 0.6072\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6532 - accuracy: 0.6213\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6659 - accuracy: 0.5901\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6484 - accuracy: 0.6233\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6432 - accuracy: 0.6363\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6607 - accuracy: 0.5996\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6764 - accuracy: 0.5724\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6888 - accuracy: 0.5323\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6828 - accuracy: 0.5449\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6842 - accuracy: 0.5449\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6815 - accuracy: 0.5590\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6703 - accuracy: 0.5838\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.6441 - accuracy: 0.6396\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 23s 285ms/step - loss: 0.6233 - accuracy: 0.6769\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.6178 - accuracy: 0.6766\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 23s 282ms/step - loss: 0.6016 - accuracy: 0.7055\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5566 - accuracy: 0.7507\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.5289 - accuracy: 0.7665\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.4859 - accuracy: 0.7834\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.4763 - accuracy: 0.7920\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 23s 285ms/step - loss: 0.4943 - accuracy: 0.7987\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.4937 - accuracy: 0.7841\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.4519 - accuracy: 0.8070\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.4423 - accuracy: 0.8192\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.3957 - accuracy: 0.8594\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.3726 - accuracy: 0.8646\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.3687 - accuracy: 0.8585\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3534 - accuracy: 0.8663\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3475 - accuracy: 0.8659\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3414 - accuracy: 0.8748\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.3351 - accuracy: 0.8710\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.3250 - accuracy: 0.8768\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.3097 - accuracy: 0.8831\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.3213 - accuracy: 0.8775\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.3037 - accuracy: 0.8812\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2761 - accuracy: 0.8925\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2780 - accuracy: 0.8894\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 23s 284ms/step - loss: 0.2773 - accuracy: 0.8920\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 23s 283ms/step - loss: 0.2739 - accuracy: 0.8931\n",
            "Score for fold 5: loss of 0.30522263050079346; accuracy of 88.70967626571655%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold4)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold4[i]} - Accuracy: {acc_per_fold4[i]}%')  \n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold4)} (+- {np.std(acc_per_fold4)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold4)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n",
        "np.savetxt('/content/drive/MyDrive/ML/Hyper_opti/dense_param256_50.csv', acc_per_fold4, delimiter=',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUftT5KSLBeN",
        "outputId": "c0115b15-b7ad-4cf1-d8ea-c077a4121fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.2634943425655365 - Accuracy: 89.11290168762207%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.354265958070755 - Accuracy: 86.82795763015747%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.2541850209236145 - Accuracy: 89.85214829444885%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.19367565214633942 - Accuracy: 92.6075279712677%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.30522263050079346 - Accuracy: 88.70967626571655%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 89.42204236984253 (+- 1.8796073359958347)\n",
            "> Loss: 0.27416872084140775\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        " \n",
        " \n",
        "# Creating dataset\n",
        "data_4 = np.array(acc_per_fold4)\n",
        "data_1 = np.array(acc_per_fold1)\n",
        "data_2 = np.array(acc_per_fold2)\n",
        "data_3 = np.array(acc_per_fold3)\n",
        "data = [data_2.flatten(), data_3.flatten(), data_1.flatten(), data_4.flatten()]\n",
        "print(data)\n",
        "fig = plt.figure()\n",
        " \n",
        "# Creating axes instance\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "#plt.ylim([60,85])\n",
        "\n",
        "txtstr = f' param 16 acc = {round(np.mean(acc_per_fold2),2)} (+- {round(np.std(acc_per_fold2),2)}) \\n param 64 acc = {round(np.mean(acc_per_fold3),2)} (+- {round(np.std(acc_per_fold3),2)}) \\n param 100 acc = {round(np.mean(acc_per_fold1),2)} (+- {round(np.std(acc_per_fold1),2)}) \\n param 256 acc = {round(np.mean(acc_per_fold4),2)} (+- {round(np.std(acc_per_fold4),2)})'\n",
        "plt.text(0.452,0.045,txtstr,fontsize=13, transform=plt.gcf().transFigure, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "bp = ax.boxplot(data, labels=[16, 64, 100,  256], showmeans=True)\n",
        "plt.xlabel('Parameter')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "\n",
        "#plt.plot(fpr2, tpr2, label='narco (AUC = {:.3f})'.format(auc1))\n",
        "#plt.legend([f'param 4 acc = {round(np.mean(acc_per_fold16),2)} (+- {round(np.std(acc_per_fold16),2)})' ,\n",
        " #           f'param 6 acc = {round(np.mean(acc_per_fold64),2)} (+- {round(np.std(acc_per_fold64),2)})' , \n",
        "  #          f'param 16 acc = {round(np.mean(acc_per_fold6),2)} (+- {round(np.std(acc_per_fold6),2)})', \n",
        "   #         f'param 64 acc = {round(np.mean(acc_per_fold4),2)} (+- {round(np.std(acc_per_fold4),2)})'], loc='lower left')\n",
        "\n",
        "\n",
        "plt.savefig(\"hyper_opti.pdf\", bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "BU49_sjEMMOj",
        "outputId": "88ca3eba-2f10-4443-845c-c74dc9871a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([78.76344323, 80.64516187, 85.34946442, 85.21505594, 79.9731195 ]), array([88.10483813, 89.24731016, 87.83602118, 87.36559153, 90.45698643]), array([90.18816948, 89.58333135, 91.59946442, 91.33064747, 89.51612711]), array([89.11290169, 86.82795763, 89.85214829, 90.36574125, 88.70967627])]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_fold3 = np.loadtxt('/content/drive/MyDrive/ML/Hyper_opti/lstm_param64_50.csv', delimiter=',')\n",
        "acc_per_fold1 = np.loadtxt('/content/drive/MyDrive/ML/Hyper_opti/lstm_param100_50.csv', delimiter=',')\n",
        "acc_per_fold2 = np.loadtxt('/content/drive/MyDrive/ML/Hyper_opti/lstm_param16_50.csv', delimiter=',')\n",
        "acc_per_fold4 = np.loadtxt('/content/drive/MyDrive/ML/Hyper_opti/lstm_param256_50.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "d4ZDyA4TWK58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}